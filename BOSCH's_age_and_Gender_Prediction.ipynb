{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wuKRqYM-L53o"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "!pip install --upgrade --no-cache-dir gdown\n",
        "!pip install pytube"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOUBSTly-68_",
        "outputId": "737101f8-d427-4dbb-ef52-d44c03171f78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id '10NaxFCpitXjtLX0rZ4M02FV0rOGRpLKI' #weights_dummy.pt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-bdjKMKut2a",
        "outputId": "7c590b3a-d610-48e7-dc79-f2321b0cec2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=10NaxFCpitXjtLX0rZ4M02FV0rOGRpLKI\n",
            "To: /weights_dummy.pt\n",
            "100% 166M/166M [00:02<00:00, 68.2MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCiLmFc-Jene",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cc5126e-58f1-4a13-99e8-586166f4feaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=10NaxFCpitXjtLX0rZ4M02FV0rOGRpLKI\n",
            "To: /weights_dummy.pt\n",
            "100% 166M/166M [00:01<00:00, 154MB/s]\n",
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1rJI17Y2u0MDmqv2qkcM6ScOYZpGE_9dd\n",
            "To: /weights.caffemodel\n",
            "100% 5.35M/5.35M [00:00<00:00, 32.4MB/s]\n",
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1M38YE0S9ZztaKK9JJ1GeBbUqIunAimRV\n",
            "To: /deploy.prototxt\n",
            "100% 28.1k/28.1k [00:00<00:00, 43.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown --id '10NaxFCpitXjtLX0rZ4M02FV0rOGRpLKI' #weights_dummy.pt\n",
        "!gdown --id '1rJI17Y2u0MDmqv2qkcM6ScOYZpGE_9dd' #weights.caffemodel - opencv face detection neural network\n",
        "!gdown --id '1M38YE0S9ZztaKK9JJ1GeBbUqIunAimRV' #deploy.prototext file for caffemodel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otrZdXl58mPH"
      },
      "source": [
        "## **Detecting humans**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfI2u3bMYWOL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "287602f1-9935-4527-eb8f-990212f7051f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU\n"
          ]
        }
      ],
      "source": [
        "## Setting the device\n",
        "\n",
        "import torch\n",
        "from math import floor, ceil\n",
        "cuda_enable = True\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda:0\")\n",
        "  print(\"GPU\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "  print(\"CPU\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRHpDpHtYWOQ"
      },
      "outputs": [],
      "source": [
        "## Importing requirements\n",
        "\n",
        "import os\n",
        "import shutil \n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from torch import nn\n",
        "from pycocotools.coco import COCO\n",
        "from torchvision import datasets, transforms, models\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from pytube import YouTube\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Applying the Augmentations\n",
        "\n",
        "def get_transforms(train):\n",
        "  trans = []\n",
        "  if train:\n",
        "    trans.append(transforms.RandomHorizontalFlip(0.5))\n",
        "    trans.append(transforms.RandomAffine(0, shear=10, scale=(0.8, 1.2)))\n",
        "    trans.append(transforms.ColorJitter(brightness=1, contrast=1, saturation=1))\n",
        "  trans.append(transforms.ToTensor())\n",
        "  return transforms.Compose(trans)"
      ],
      "metadata": {
        "id": "3QJ3pc65j5OX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Uncomment the following cells to train the model again, in that case comment out/do not run the 'Training is skipped and model is loaded with pretrained weights' section.**"
      ],
      "metadata": {
        "id": "NmELWRyWlK4z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "!git clone https://github.com/Chang-Chia-Chi/Pedestrian.git\n",
        "!git clone https://github.com/Chang-Chia-Chi/Final-Project.git\n",
        "'''"
      ],
      "metadata": {
        "id": "1MCARajejnE2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "aea19cec-a9c4-4714-c2d6-1432cce4adbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n!git clone https://github.com/Chang-Chia-Chi/Pedestrian.git\\n!git clone https://github.com/Chang-Chia-Chi/Final-Project.git\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3te59BNC9c8u"
      },
      "source": [
        "### **Training is skipped and model is loaded with pretrained weights**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-lnDZIEYWOg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "62bd0fc0b29a4e18a3f11bafb7f1ce3b",
            "dd09ef986c0642b187cbe2259c51bf2c",
            "3d9be96fcf0e496480f35dea60f0134d",
            "7087bb1e3cec40a3a98d52ad17234194",
            "b345a3d40e334fcd95029e638165926f",
            "be7e2ad889814fa1a471c674f354ce9e",
            "a22ab99c0d3b4ea98e76cd7b412ec14f",
            "58c9297082224bc09b280cb23e0ebcfa",
            "424334f17f594a44a9c76dc69818c92d",
            "da4254594e5543728e4116dffd56112c",
            "37189771590c4287a13bf80249e43fe6"
          ]
        },
        "outputId": "9f4e0c55-079f-4c02-b6b9-99548e97939f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/160M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "62bd0fc0b29a4e18a3f11bafb7f1ce3b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "## Making the model for body detection\n",
        "\n",
        "model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "num_classes = 2\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "model = model.to(device)\n",
        "model.load_state_dict(torch.load('weights_dummy.pt'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owo_Hh919jE5"
      },
      "source": [
        "### **Cropping out the persons**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5N7oSeZkYWOu"
      },
      "outputs": [],
      "source": [
        "## Image convert\n",
        "\n",
        "def im_convert(tensor):\n",
        "    image = tensor.clone().detach().cpu().numpy()\n",
        "    image = image.transpose(1, 2, 0)\n",
        "    image = image * np.array((0.5, 0.5, 0.5)) + np.array((0.5, 0.5, 0.5))\n",
        "    image = image.clip(0, 1)\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Making a dictinary for the purpose of getting the bounding box of the extracted image from the video\n",
        "all_boxes = {}"
      ],
      "metadata": {
        "id": "UW6vjErrLNW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4gK5us6eWnZs"
      },
      "outputs": [],
      "source": [
        "## Geting the Bounding box of the person from the image frame\n",
        "\n",
        "def get_bbox(frame,model,x,k):\n",
        "  img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "  val_tran = get_transforms(train=False)\n",
        "  im_pil = Image.fromarray(img)\n",
        "  im_pil = val_tran(im_pil)\n",
        "\n",
        "  image = im_pil.to(device)\n",
        "  model_val = model.eval()\n",
        "  output = model([image])\n",
        "\n",
        "  scores = output[0]['scores'].detach().cpu().numpy()\n",
        "  num_people = len(scores[scores > 0.7])\n",
        "\n",
        "  boxes = output[0]['boxes'].detach().cpu().numpy()\n",
        "  boxes = boxes[:num_people]\n",
        "\n",
        "  scores = scores[:num_people]\n",
        "  indices =  torch.ops.torchvision.nms(torch.Tensor(boxes),torch.Tensor(scores),0.25).tolist()\n",
        "  \n",
        "  container = []\n",
        "  new_scores = []\n",
        "  size = 0\n",
        "  if len(indices)==0:\n",
        "    return \n",
        "  for i in indices:\n",
        "    container.append([boxes[i][0], boxes[i][1], boxes[i][2], boxes[i][3]])\n",
        "    new_scores.append(scores[i])\n",
        "    all_boxes['m1_cropped/B_{f0}_{f1}_BC{f2}.jpg'.format(f0=k,f1=int(x),f2=i)] = [boxes[i][0], boxes[i][1], boxes[i][2], boxes[i][3]]\n",
        "    size+=1\n",
        "  for i in range(len(indices)):\n",
        "    crop = img[floor(container[i][1]):ceil(container[i][3]), floor(container[i][0]): ceil(container[i][2])]\n",
        "    resize = cv2.resize(crop, (112,112))\n",
        "    resize = cv2.cvtColor(resize, cv2.COLOR_BGR2RGB)\n",
        "    cv2.imwrite(\"m1_cropped/B_{f0}_{f1}_BC{f2}.jpg\".format(f0=k,f1=int(x),f2=i), resize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwbmCpL6-eJv"
      },
      "source": [
        "### **Get the input video/image to crop out the persons**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTy2vz3ygXcD"
      },
      "outputs": [],
      "source": [
        "## Function for getting the image from the input video\n",
        "\n",
        "def videoDataProcessing(model,path = 'VideoData.mp4',k=0):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    i = 1\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if ret and i%20==0:\n",
        "            get_bbox(frame,model,i/20,k)\n",
        "            print(i)\n",
        "        elif not ret:\n",
        "            break\n",
        "        i+=1\n",
        "        if cv2.waitKey(20) & 0xFF == ord('q'):\n",
        "            break\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H01sm7fXdfkv"
      },
      "outputs": [],
      "source": [
        "## Downloading the video from youtube\n",
        "\n",
        "def get_from_YouTube(link):\n",
        "    Link = link\n",
        "    yt = YouTube(Link)\n",
        "    stream = yt.streams.filter(res='720p').first()\n",
        "    stream.download(filename='VideoData.mp4')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Taking the inputs from the user.**"
      ],
      "metadata": {
        "id": "N3DgCYRLlxQC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMFhkw8OeezW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9964b93-e974-40c1-a3bf-546694ebc58b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter 0 for uploading a video file.\n",
            "Enter 1 for uploading images.\n",
            "0\n",
            "Enter 0 for uploading youtube video.\n",
            "Enter 1 for uploading local files.\n",
            "0\n",
            "Enter youTube video link.\n",
            "https://youtu.be/HJ1kfhcz8jY\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n",
            "40\n",
            "60\n",
            "80\n",
            "100\n",
            "120\n",
            "140\n",
            "160\n",
            "180\n",
            "200\n",
            "220\n",
            "240\n",
            "260\n",
            "280\n",
            "300\n",
            "320\n",
            "340\n",
            "360\n",
            "380\n"
          ]
        }
      ],
      "source": [
        "## Taking the input from the user, either need to provide the video or the image\n",
        "\n",
        "print(\"Enter 0 for uploading a video file.\\nEnter 1 for uploading images.\")\n",
        "entry = int(input())\n",
        "if os.path.isdir('m1_cropped'):\n",
        "    shutil.rmtree('m1_cropped')\n",
        "os.mkdir('m1_cropped')\n",
        "if entry==0:\n",
        "    print(\"Enter 0 for uploading youtube video.\\nEnter 1 for uploading local files.\")\n",
        "    entry2 = int(input())\n",
        "    if entry2==0:\n",
        "        print(\"Enter youTube video link.\")\n",
        "        link = input()\n",
        "        get_from_YouTube(link)\n",
        "        videoDataProcessing(model)\n",
        "    else:\n",
        "        uploaded = files.upload()\n",
        "        if os.path.isdir('Videos'):\n",
        "            shutil.rmtree('Videos')\n",
        "        os.mkdir('Videos')\n",
        "        upload_folder = 'Videos'\n",
        "        k=0\n",
        "        for filename in uploaded.keys():\n",
        "            dst_path = os.path.join(upload_folder, filename)\n",
        "            print(f'move {filename} to {dst_path}')\n",
        "            shutil.move(filename, dst_path)\n",
        "            print(dst_path)\n",
        "            videoDataProcessing(model,dst_path,k)\n",
        "            k+=1\n",
        "else:\n",
        "    uploaded = files.upload()\n",
        "    if os.path.isdir('Images'):\n",
        "        shutil.rmtree('Images')\n",
        "    os.mkdir('Images')\n",
        "    upload_folder = 'Images'\n",
        "    i=0\n",
        "    k=0\n",
        "    for filename in uploaded.keys():\n",
        "        dst_path = os.path.join(upload_folder, filename)\n",
        "        print(f'move {filename} to {dst_path}')\n",
        "        shutil.move(filename, dst_path)\n",
        "        frame = cv2.imread(dst_path)\n",
        "        get_bbox(frame,model,i,k)\n",
        "        i+=1\n",
        "        k+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nXlaUHLVBRQ"
      },
      "source": [
        "### **OpenCV CaffeModel for face recognition**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yXVhRy22e3F"
      },
      "outputs": [],
      "source": [
        "# Define paths\n",
        "base_dir = ''\n",
        "prototxt_path = os.path.join(base_dir + 'deploy.prototxt')\n",
        "caffemodel_path = os.path.join(base_dir + 'weights.caffemodel')\n",
        "\n",
        "# Read the model\n",
        "\n",
        "model = cv2.dnn.readNetFromCaffe(prototxt_path, caffemodel_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCefEz9G4oRO"
      },
      "outputs": [],
      "source": [
        "# Create directory 'updated_images' and if it exists then delete it first.\n",
        "\n",
        "if os.path.isdir('updated_images'):\n",
        "    shutil.rmtree('updated_images')\n",
        "os.makedirs('updated_images')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKRJkz9M40Th"
      },
      "outputs": [],
      "source": [
        "# Create directory 'faces' and if it exists then delete it first.\n",
        "\n",
        "\n",
        "if os.path.isdir('faces'):\n",
        "    shutil.rmtree('faces')\n",
        "os.makedirs('faces')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SoNs38Gr-s7J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "outputId": "ece095ed-f439-4d44-965e-5773a048e5a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image B_0_113_BC1.jpg converted successfully\n",
            "Image B_0_121_BC0.jpg converted successfully\n",
            "Image B_0_22_BC0.jpg converted successfully\n",
            "Image B_0_156_BC0.jpg converted successfully\n",
            "Image B_0_33_BC0.jpg converted successfully\n",
            "Image B_0_211_BC0.jpg converted successfully\n",
            "Image B_0_80_BC0.jpg converted successfully\n",
            "Image B_0_253_BC0.jpg converted successfully\n",
            "Image B_0_29_BC1.jpg converted successfully\n",
            "Image B_0_206_BC0.jpg converted successfully\n",
            "Image B_0_139_BC0.jpg converted successfully\n",
            "Image B_0_250_BC0.jpg converted successfully\n",
            "Image B_0_63_BC0.jpg converted successfully\n",
            "Image B_0_163_BC0.jpg converted successfully\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-b8f85fd7a81b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m       \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m       \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstartY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mendY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstartX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mendX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m       \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'faces/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.1.2) /io/opencv/modules/imgcodecs/src/loadsave.cpp:715: error: (-215:Assertion failed) !_img.empty() in function 'imwrite'\n"
          ]
        }
      ],
      "source": [
        "# Loop through all images and save images with marked faces\n",
        "\n",
        "for file in os.listdir(base_dir + 'm1_cropped'):\n",
        "  file_name, file_extension = os.path.splitext(file)\n",
        "  image = cv2.imread(base_dir + 'm1_cropped/' + file)\n",
        "\n",
        "  (h, w) = image.shape[:2]\n",
        "  blob = cv2.dnn.blobFromImage(cv2.resize(image, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n",
        "\n",
        "  model.setInput(blob)\n",
        "  detections = model.forward()\n",
        "\n",
        "  # Create frame around face\n",
        "\n",
        "  for i in range(0, detections.shape[2]):\n",
        "    box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "    (startX, startY, endX, endY) = box.astype(\"int\")\n",
        "\n",
        "    confidence = detections[0, 0, i, 2]\n",
        "\n",
        "    # If confidence > 0.5, show box around face\n",
        "\n",
        "    if (confidence > 0.5):\n",
        "      cv2.rectangle(image, (startX, startY), (endX, endY), (255, 255, 255), 2)\n",
        "\n",
        "  cv2.imwrite(base_dir + 'updated_images/' + file, image)\n",
        "  print(\"Image \" + file + \" converted successfully\")\n",
        "\n",
        "  # Identify each face\n",
        "\n",
        "  count=0\n",
        "  for i in range(0, detections.shape[2]):\n",
        "    box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "    (startX, startY, endX, endY) = box.astype(\"int\")\n",
        "\n",
        "    confidence = detections[0, 0, i, 2]\n",
        "\n",
        "    # If confidence > 0.5, save it as a separate file\n",
        "    \n",
        "    if (confidence > 0.5):\n",
        "      count += 1\n",
        "      frame = image[startY:endY, startX:endX]\n",
        "      cv2.imwrite(base_dir + 'faces/' + file, frame)                                "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpX1KGCsY7mQ"
      },
      "source": [
        "# **M2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnpnrLfMV2jU"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "# Clone Real-ESRGAN and enter the Real-ESRGAN\n",
        "!git clone https://github.com/xinntao/Real-ESRGAN.git\n",
        "%cd Real-ESRGAN\n",
        "# Set up the environment\n",
        "!pip install basicsr\n",
        "!pip install facexlib\n",
        "!pip install gfpgan\n",
        "!pip install -r requirements.txt\n",
        "!python setup.py develop\n",
        "# Download the pre-trained model\n",
        "!wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth -P experiments/pretrained_models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pr9am3HWtvH"
      },
      "source": [
        "### **Upload m1_cropped, Upload the body shape images to be processed by Real-ESRGAN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAlPhlprdBR6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "2d828433-2b16-466e-b754-c1f3ff18f007"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-7da96a7689f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mos\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0misfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0monlyfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34mf\"/content/m1_cropped/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmypath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmypath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/m1_cropped'"
          ]
        }
      ],
      "source": [
        "## Getting the path of the cropped image\n",
        "\n",
        "mypath = '/content/m1_cropped'\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "onlyfiles = [f\"/content/m1_cropped/\"+f for f in listdir(mypath) if isfile(join(mypath, f))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1tiyMZJW5td"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "upload_folder = 'upload'\n",
        "result_folder = 'results'\n",
        "\n",
        "if os.path.isdir(upload_folder):\n",
        "    shutil.rmtree(upload_folder)\n",
        "if os.path.isdir(result_folder):\n",
        "    shutil.rmtree(result_folder)\n",
        "os.mkdir(upload_folder)\n",
        "os.mkdir(result_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jmh0VZH1cxl8"
      },
      "outputs": [],
      "source": [
        "for f in onlyfiles:\n",
        "    shutil.copy(f, '/content/Real-ESRGAN/upload/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqiEtaOYXnrt"
      },
      "source": [
        "3. Inference for body images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTVL4m_zXqBU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8629179-67bc-4a7c-fee3-13af8381fe54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://github.com/xinntao/facexlib/releases/download/v0.1.0/detection_Resnet50_Final.pth\" to /usr/local/lib/python3.7/dist-packages/facexlib/weights/detection_Resnet50_Final.pth\n",
            "\n",
            "100% 104M/104M [00:03<00:00, 31.3MB/s]\n",
            "Downloading: \"https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth\" to /usr/local/lib/python3.7/dist-packages/gfpgan/weights/GFPGANv1.3.pth\n",
            "\n",
            "100% 332M/332M [00:11<00:00, 31.5MB/s]\n",
            "Testing 0 B_0_0_BC0\n",
            "Testing 1 B_0_0_BC1\n"
          ]
        }
      ],
      "source": [
        "# if it is out of memory, try to use the `--tile` option\n",
        "# We upsample the image with the scale factor X3.5\n",
        "!python inference_realesrgan.py -n RealESRGAN_x4plus --i upload --outscale 2 --half --face_enhance\n",
        "# Arguments\n",
        "# -n, --model_name: Model names\n",
        "# -i, --input: input folder or image\n",
        "# --outscale: Output scale, can be arbitrary scale factore. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FewtPuQnaYaZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a938d095-7e4e-4741-b793-dd132798f82e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "## Some steps for the purpose of getting super resoluted image\n",
        "\n",
        "!mv '/content/Real-ESRGAN/results' '/content/'\n",
        "%cd ..\n",
        "if os.path.isdir('/content/body_images'):\n",
        "    shutil.rmtree('/content/body_images')\n",
        "os.rename(\"results\", \"body_images\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkHTENUBbHut"
      },
      "source": [
        "3. Upload faces\n",
        "<br>\n",
        "Upload the face shape images to be processed by Real-ESRGAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0O-hk6ebHuu"
      },
      "outputs": [],
      "source": [
        "# !mkdir '/content/Real-ESRGAN/Real_ESRGAN'\n",
        "# !mv '/content/m1_cropped' '/content/Real-ESRGAN'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTNQdN6lbHuu"
      },
      "outputs": [],
      "source": [
        "mypath = '/content/faces'\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "onlyfiles = [f\"/content/faces/\"+f for f in listdir(mypath) if isfile(join(mypath, f))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-sLiojmbHuv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "upload_folder = 'upload'\n",
        "result_folder = 'results'\n",
        "\n",
        "if os.path.isdir(upload_folder):\n",
        "    shutil.rmtree(upload_folder)\n",
        "if os.path.isdir(result_folder):\n",
        "    shutil.rmtree(result_folder)\n",
        "os.mkdir(upload_folder)\n",
        "os.mkdir(result_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fl1e2CrEbHuv"
      },
      "outputs": [],
      "source": [
        "for f in onlyfiles:\n",
        "    shutil.copy(f, '/content/Real-ESRGAN/upload/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5lDXyBZbHuv"
      },
      "source": [
        "# 4. Inference for face images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArCwh81fbHuw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f3b5bfb-5a70-4712-ebe3-0801a23d9fcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file 'inference_realesrgan.py': [Errno 2] No such file or directory\n"
          ]
        }
      ],
      "source": [
        "# if it is out of memory, try to use the `--tile` option\n",
        "# We upsample the image with the scale factor X3.5\n",
        "!python inference_realesrgan.py -n RealESRGAN_x4plus --i upload --outscale 2 --half --face_enhance\n",
        "# Arguments\n",
        "# -n, --model_name: Model names\n",
        "# -i, --input: input folder or image\n",
        "# --outscale: Output scale, can be arbitrary scale factore. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AsZW94eHbHuw"
      },
      "outputs": [],
      "source": [
        "!mv '/content/Real-ESRGAN/results' '/content/'\n",
        "%cd ../content/\n",
        "os.rename(\"results\",\"face_images\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3Ja6rPnV5z8"
      },
      "source": [
        "# **M3**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_xACXOqWJHC"
      },
      "outputs": [],
      "source": [
        "## Integrating the M3 Model with M1 and M2\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split as tts\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from tqdm.auto import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Slc1AQ8KWP8d"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zuV-pUq4Pq_u"
      },
      "outputs": [],
      "source": [
        "class ConvNeuralNet(nn.Module):\n",
        "\t#  Determine what layers and their order in CNN object \n",
        "    def __init__(self, num_classes):\n",
        "        super(ConvNeuralNet, self).__init__()\n",
        "        self.conv_layer1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.max_pool1 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
        "\n",
        "        self.conv_layer2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.max_pool2 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
        "\n",
        "        self.conv_layer3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.max_pool3 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
        "\n",
        "        self.flat = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(23040, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 1024)\n",
        "        self.fc3 = nn.Linear(1024, 5)\n",
        "    \n",
        "    def forward(self, x):\n",
        "\n",
        "        out = self.conv_layer1(x)\n",
        "        out = self.relu1(out)\n",
        "        out = self.max_pool1(out)\n",
        "\n",
        "        out = self.conv_layer2(out)\n",
        "        out = self.relu2(out)\n",
        "        out = self.max_pool2(out)\n",
        "                \n",
        "        out = self.conv_layer3(out)\n",
        "        out = self.relu3(out)\n",
        "        out = self.max_pool3(out)\n",
        "        \n",
        "        out = self.flat(out)\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.fc3(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Fy5EbXg99HJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0688e29d-f052-4fca-ca83-955b46477bb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Access denied with the following error:\n",
            "\n",
            " \tCannot retrieve the public link of the file. You may need to change\n",
            "\tthe permission to 'Anyone with the link', or have had many accesses. \n",
            "\n",
            "You may still be able to access the file from the browser:\n",
            "\n",
            "\t https://drive.google.com/uc?id=1-iH5j9dfS6EuSClH1EdAd8gKFN_5U4Ln \n",
            "\n",
            "The model could not be loaded\n"
          ]
        }
      ],
      "source": [
        "## Making the code\n",
        "\n",
        "model = ConvNeuralNet(5)\n",
        "!gdown --id '1-iH5j9dfS6EuSClH1EdAd8gKFN_5U4Ln'\n",
        "\n",
        "try:\n",
        "  model.load_state_dict(torch.load('/content/0.757543.pth', map_location=torch.device('cpu')))\n",
        "except:\n",
        "  print(\"The model could not be loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDAcsHj9QEI7"
      },
      "outputs": [],
      "source": [
        "scratch_model = nn.Sequential(*list(model.children())[:-3])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Age and Gender mixed Neural Network"
      ],
      "metadata": {
        "id": "V3RWBfkRlqwC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qw7u0jD_Quf6"
      },
      "outputs": [],
      "source": [
        "class AgeGenderPrediction_body(nn.Module):\n",
        "    \n",
        "    def __init__(self, age_labels, base_model = None):\n",
        "      super().__init__()\n",
        "\n",
        "      if base_model is None:\n",
        "        self.base_model = vgg16()\n",
        "      else:\n",
        "        self.base_model = base_model\n",
        "      self.gender_classification = nn.Sequential(\n",
        "                                                  nn.Linear(in_features = 23040, out_features = 512, bias = True),\n",
        "                                                  nn.ReLU(inplace = True),\n",
        "                                                  nn.Linear(in_features = 512, out_features = 256, bias = True),\n",
        "                                                  nn.ReLU(inplace=True),\n",
        "                                                  nn.Linear(in_features = 256, out_features = 2, bias = True),\n",
        "                                              )\n",
        "      \n",
        "      self.age_classification = nn.Sequential(\n",
        "                                                  nn.Linear(in_features = 23040, out_features = 512, bias = True),\n",
        "                                                  nn.ReLU(inplace = True),\n",
        "                                                  nn.Linear(in_features = 512, out_features = 256, bias = True),\n",
        "                                                  nn.ReLU(inplace=True),\n",
        "                                                  nn.Linear(in_features = 256, out_features = age_labels, bias = True),\n",
        "                                              )\n",
        "\n",
        "    def forward(self, x):\n",
        "      base_output = self.base_model(x)\n",
        "      base_output = base_output.view(base_output.shape[0], -1)\n",
        "      gender = self.gender_classification(base_output)\n",
        "      age = self.age_classification(base_output)\n",
        "\n",
        "      return age, gender"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Training and Validation of (Age + Gender) Model on body_images "
      ],
      "metadata": {
        "id": "9nZnM6d0mCfM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P93xAcycrRmr"
      },
      "outputs": [],
      "source": [
        "path = '/content/body_images'\n",
        "entry = os.listdir(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DA4FR1ZhsgKv"
      },
      "outputs": [],
      "source": [
        "class Test_Prepare(Dataset):\n",
        "  \"\"\"\n",
        "  The Class will act as the container for our dataset. It will take your dataframe, the root path, and also the transform function for transforming the dataset.\n",
        "  \"\"\"\n",
        "  def __init__(self, base_path,entry, transform=None):\n",
        "    self.base_path = base_path\n",
        "    self.entry = entry\n",
        "    self.transform = transform\n",
        "  def __len__(self):\n",
        "    # Return the length of the dataset\n",
        "    return len(self.entry)\n",
        "  def __getitem__(self, idx):\n",
        "    # Return the observation based on an index. Ex. dataset[0] will return the first element from the dataset, in this case the image and the label.\n",
        "    if torch.is_tensor(idx):\n",
        "      idx = idx.tolist()    \n",
        "    img_name = os.path.join(self.base_path, str(self.entry[idx]))\n",
        "    image = Image.open(img_name)\n",
        "    if self.transform:\n",
        "      image = self.transform(image)\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwEH0wOPsuAk"
      },
      "outputs": [],
      "source": [
        "test_dataset = Test_Prepare(path, entry, transform = transforms.Compose([transforms.ToTensor(), transforms.Resize((160, 60))]))\n",
        "dataloader = torch.utils.data.DataLoader(test_dataset, batch_size = 16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8JFO2xvp0h7g"
      },
      "outputs": [],
      "source": [
        "#model define\n",
        "model_body = AgeGenderPrediction_body(5, scratch_model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id '1-ckkov-5MHi2s5yFK2yvshzubNfGvVom'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STmXUGD4Ew2q",
        "outputId": "1fac87aa-6a6e-40a3-f824-f3c39107b2ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-ckkov-5MHi2s5yFK2yvshzubNfGvVom\n",
            "To: /content/0.668123.pth\n",
            "100% 96.9M/96.9M [00:01<00:00, 96.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import weights of age and gender from body images trained model in PETA datset folder.\n",
        "#load weights\n",
        "model_body.load_state_dict(torch.load('/content/0.668123.pth', map_location=torch.device('cpu')))"
      ],
      "metadata": {
        "id": "6EagZGnuGuUT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aaa261a2-f5fe-466e-fe81-aa26c542940d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "siWxuFqP0373"
      },
      "outputs": [],
      "source": [
        "## Defining the test function for output\n",
        "\n",
        "def test(model, dataloaders, device):\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "  \n",
        "  image = []\n",
        "  prediction_age = []\n",
        "  prediction_gender = []\n",
        "\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for inputs in tqdm(dataloaders):\n",
        "          inputs = inputs.to(device)\n",
        "\n",
        "          image.extend(inputs.detach().cpu().numpy().tolist())\n",
        "          age, gender = model(inputs)\n",
        "          _, preds_age = torch.max(age, 1)\n",
        "          _, preds_gender = torch.max(gender, 1)\n",
        "          \n",
        "          prediction_age.extend(preds_age.detach().cpu().numpy().tolist())\n",
        "          prediction_gender.extend(preds_gender.detach().cpu().numpy().tolist())\n",
        "\n",
        "  return image,prediction_age,prediction_gender"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0y3U0eoP1guA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "c7cf955f754b4a309a46f40f1784f030",
            "495b195f78364a08895a149d2cfd06eb",
            "a43e3e749b8949829c0813649fdf1a1c",
            "95c80c32eb934a8b910cb76e4158c3ba",
            "82dbc0dff7e340fab19e495f73373d8e",
            "bba2717473564d679f6be144972a49a4",
            "fed6d7567b8c45628b791cf09fc6df5f",
            "24fd2965fc564c169378e9f4fb829822",
            "bb4477cbc85444c69d07dfbb87db7951",
            "896db197c10f4036a783d1544ac7ba07",
            "220f6cd7b91e4dcaa0b8716dc321caec"
          ]
        },
        "outputId": "2f9506b4-d662-4b05-e568-a66f66bc5d42"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c7cf955f754b4a309a46f40f1784f030"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "## Testing the model\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
        "image, preds_age, preds_gender = test(model_body, dataloader, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RdZWht6KCkQ"
      },
      "outputs": [],
      "source": [
        "## Some plots\n",
        "\n",
        "images=[]\n",
        "for i in range(len(preds_age)):\n",
        "    images.append(np.transpose(np.array(image[i]), (1,2,0)))\n",
        "\n",
        "images = np.array(images)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## vgg16\n"
      ],
      "metadata": {
        "id": "GyAtVF8wJNDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vgg16 = models.vgg16(pretrained=True)\n",
        "vgg16 = nn.Sequential(*list(vgg16.children())[:-1])"
      ],
      "metadata": {
        "id": "iLWjc9aPIOsg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "aff15737fb004a728124e74a92bce2ef",
            "31aac39166224797bd90dc46cb90db91",
            "a33c3bc55a584aa399ef83c8cc238fa5",
            "58eb380d12324bdfa431cf4492cc9f54",
            "e860de30ea764e5898babd6cba5fedde",
            "888f2e5574a84de1aa583a3436d3f4bf",
            "999079e7fa344d96b854afba59fc323f",
            "19482c64aacb4bb6b7fd0f25578e2e25",
            "cce1b7fa6af14afba35e64ae70022bc2",
            "44c91eb636544d2cb95d66c16feb1284",
            "3830dafb2f5b494b856042cc8abf3428"
          ]
        },
        "outputId": "bdf0f88d-f396-48e7-fb50-856122ddec51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/528M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aff15737fb004a728124e74a92bce2ef"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AgeGenderPrediction_face(nn.Module):\n",
        "    \n",
        "    def __init__(self, age_labels, base_model = None):\n",
        "      super().__init__()\n",
        "\n",
        "      if base_model is None:\n",
        "        self.base_model = vgg16()\n",
        "      else:\n",
        "        self.base_model = base_model\n",
        "      self.gender_classification = nn.Sequential(\n",
        "                                                  nn.Linear(in_features = 25088, out_features = 2, bias = True)\n",
        "                                              )\n",
        "      \n",
        "      self.age_classification = nn.Sequential(\n",
        "                                                  nn.Linear(in_features = 25088, out_features =age_labels, bias = True),\n",
        "                                                  \n",
        "                                              )\n",
        "\n",
        "    def forward(self, x):\n",
        "      base_output = self.base_model(x)\n",
        "      base_output = base_output.view(base_output.shape[0], -1)\n",
        "      gender = self.gender_classification(base_output)\n",
        "      age = self.age_classification(base_output)\n",
        "\n",
        "      return age, gender"
      ],
      "metadata": {
        "id": "nVJ_SG3wXAlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VGG16 = AgeGenderPrediction_face(24, vgg16)"
      ],
      "metadata": {
        "id": "apJ-dvpDpN1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id '1-FcOKMmwqdne3tsLJMYdCW18z888Jcxh'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLcAQLqzE8Sm",
        "outputId": "123aae7a-2074-48b4-db0d-1f744dcc8b74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Access denied with the following error:\n",
            "\n",
            " \tCannot retrieve the public link of the file. You may need to change\n",
            "\tthe permission to 'Anyone with the link', or have had many accesses. \n",
            "\n",
            "You may still be able to access the file from the browser:\n",
            "\n",
            "\t https://drive.google.com/uc?id=1-FcOKMmwqdne3tsLJMYdCW18z888Jcxh \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "VGG16.load_state_dict(torch.load('/content/0.231848.pth', map_location=torch.device('cpu')))"
      ],
      "metadata": {
        "id": "1jSy2UWLhHjG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "325ccc2c-fb34-40a1-e98b-ba8e5c10e1ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Detaching the classification part\n",
        "\n",
        "model_body = nn.Sequential(*(list(model_body.children())[0]))"
      ],
      "metadata": {
        "id": "3D-kLEDHaNPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combining the facial as well as the body feature"
      ],
      "metadata": {
        "id": "O9ehruV4t01B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "class CombinedModelM3(nn.Module):\n",
        "  def __init__(self, model_facial, model_body, num_labels_age = 5):\n",
        "    super().__init__()\n",
        "    # Note that the last layer for classification should be removed in both the models\n",
        "\n",
        "    self.face_ext = model_facial\n",
        "    self.body_ext = model_body\n",
        "\n",
        "    self.gender_classification = nn.Sequential(\n",
        "                                                  nn.Linear(in_features = 91648, out_features = 512, bias = True),\n",
        "                                                  nn.ReLU(inplace = True),\n",
        "                                                  nn.Linear(in_features = 512, out_features = 256, bias = True),\n",
        "                                                  nn.ReLU(inplace=True),\n",
        "                                                  nn.Linear(in_features = 256, out_features = 2, bias = True),\n",
        "                                              )\n",
        "      \n",
        "    self.age_classification = nn.Sequential(\n",
        "                                                  nn.Linear(in_features = 91648, out_features = 512, bias = True),\n",
        "                                                  nn.ReLU(inplace = True),\n",
        "                                                  nn.Linear(in_features = 512, out_features = 256, bias = True),\n",
        "                                                  nn.ReLU(inplace=True),\n",
        "                                                  nn.Linear(in_features = 256, out_features = num_labels_age, bias = True),\n",
        "                                              )\n",
        "    \n",
        "\n",
        "  def forward(self, face, body):\n",
        "\n",
        "    face_ft = self.face_ext(face).view(face.shape[0], -1)\n",
        "    body_ft = self.body_ext(body).view(body.shape[0], -1)\n",
        "\n",
        "    combined_ft = torch.cat([face_ft, body_ft], axis = -1)\n",
        "\n",
        "    gender = self.gender_classification(combined_ft)\n",
        "    age = self.age_classification(combined_ft)\n",
        "\n",
        "    return age, gender"
      ],
      "metadata": {
        "id": "bTK30EI0t5e8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training, eval and test function of CombinedModelM3 "
      ],
      "metadata": {
        "id": "FaaKihEwtxlC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taqveAwQtv9M"
      },
      "outputs": [],
      "source": [
        "def train(model, dataloaders, criterion_1, criterion_2, optimizer, device, num_epochs=1):\n",
        "    model.train()\n",
        "    # Define threshold\n",
        "    t_age_acc = 0.7\n",
        "    t_gender_acc = 0.7\n",
        "\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    for inputs_face, inputs_body, labels_age, labels_gender in tqdm(dataloaders):\n",
        "      optimizer.zero_grad()\n",
        "      inputs_face = inputs_face.to(device)\n",
        "      inputs_body = inputs_body.to(device)\n",
        "      labels_gender = labels_gender.to(device)\n",
        "      labels_age = labels_age.to(device)\n",
        "\n",
        "      age, gender = model(inputs_face, inputs_body)\n",
        "      loss_1 = criterion_1(age, labels_age)\n",
        "      loss_2 = criterion_2(gender, labels_gender)\n",
        "      _, preds_age = torch.max(age, 1)\n",
        "      _, preds_gender = torch.max(gender, 1)\n",
        "\n",
        "\n",
        "      running_corrects+=torch.sum((preds_age == labels_age.data) & (preds_gender == labels_gender.data))\n",
        "\n",
        "      age_accuracy = torch.sum((preds_age == labels_age.data))/len(inputs_face)\n",
        "      gender_accuracy = torch.sum((preds_gender == labels_gender.data))/len(inputs_face)\n",
        "\n",
        "      ans = (bool(age_accuracy.item() <=t_age_acc)) and (bool(gender_accuracy.item() <= t_gender_acc) )\n",
        "      if(bool(ans)):\n",
        "          loss = 0.5*(loss_1 + loss_2)\n",
        "\n",
        "      elif(bool(age_accuracy.item() <=t_age_acc)):\n",
        "        \n",
        "        loss = 0.8*loss_1 + 0.2*loss_2\n",
        "\n",
        "      else:\n",
        "         loss = 0.2*loss_1 + 0.8*loss_2\n",
        "      \n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      running_loss+=loss.item()*inputs_face.size(0)\n",
        "\n",
        "    epoch_loss = running_loss/len(dataloaders.dataset)\n",
        "    epoch_acc = running_corrects.double()/len(dataloaders.dataset)\n",
        "    \n",
        "    return epoch_loss, epoch_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3Yyjb5Ytv9N"
      },
      "outputs": [],
      "source": [
        "def eval(model, dataloaders, criterion, device, num_epochs=1):\n",
        "  model.eval()\n",
        "  \n",
        "\n",
        "  # Define threshold\n",
        "  t_age_acc = 0.7\n",
        "  t_gender_acc = 0.7  \n",
        "  with torch.no_grad():\n",
        "\n",
        "      running_loss = 0.0\n",
        "      running_corrects = 0\n",
        "      for inputs_face, inputs_body, labels_age, labels_gender in tqdm(dataloaders):\n",
        "          inputs_face = inputs_face.to(device)\n",
        "          inputs_body = inputs_body.to(device)\n",
        "          labels_age = labels_age.to(device)\n",
        "          labels_gender = labels_gender.to(device)\n",
        "\n",
        "          age, gender = model(inputs_face, inputs_body)\n",
        "          loss_1 = criterion(age, labels_age)\n",
        "          loss_2 = criterion(gender, labels_gender)\n",
        "          loss = loss_1 + loss_2\n",
        "          _, preds_age = torch.max(age, 1)\n",
        "          _, preds_gender = torch.max(gender, 1)\n",
        "\n",
        "\n",
        "          running_corrects+=torch.sum((preds_age == labels_age.data) & (preds_gender == labels_gender.data))\n",
        "\n",
        "          age_accuracy = torch.sum((preds_age == labels_age.data))/len(inputs_face)\n",
        "          gender_accuracy = torch.sum((preds_gender == labels_gender.data))/len(inputs_face)\n",
        "\n",
        "          ## Based upon the accuracy\n",
        "          ans = (bool(age_accuracy.item() <=t_age_acc)) and (bool(gender_accuracy.item() <= t_gender_acc))\n",
        "          \n",
        "          if(bool(ans)):\n",
        "              loss = 0.5*(loss_1 + loss_2)\n",
        "\n",
        "          elif(bool(age_accuracy.item() <=t_age_acc)):\n",
        "            loss = 0.8*loss_1 + 0.2*loss_2\n",
        "\n",
        "          else:\n",
        "            loss = 0.2*loss_1 + 0.8*loss_2\n",
        "\n",
        "          running_loss+=loss.item()*inputs_face.size(0)\n",
        "\n",
        "      epoch_loss = running_loss/len(dataloaders.dataset)\n",
        "      epoch_acc = running_corrects.double()/len(dataloaders.dataset)\n",
        "\n",
        "\n",
        "  return epoch_loss, epoch_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kx_5GmSPtv9N"
      },
      "outputs": [],
      "source": [
        "def run(train_dl, val_dl, epochs):\n",
        "    model_combined = CombinedModelM3(vgg16, model_body)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
        "    model_combined.to(device)\n",
        "\n",
        "    accuracies = []\n",
        "    losses = []\n",
        "\n",
        "    best_acc = 0.0\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        train_loss, train_acc = train(model_combined, train_dl, criterion, criterion, optimizer, device)\n",
        "        val_loss, val_acc = eval(model_combined, val_dl, criterion, device)\n",
        "\n",
        "        if val_acc > best_acc:\n",
        "          best_acc =val_acc\n",
        "          torch.save(model.state_dict(), os.path.join('/content/drive/MyDrive/Inter IIT/', '{0:0=2f}.pth'.format(best_acc)))\n",
        "\n",
        "        print(\"Train loss for epoch {}\".format(epoch+1) , \"is \", train_loss ,\" and train accuracy is \", train_acc)\n",
        "        print(\"Validation loss for epoch {}\".format(epoch+1) , \"is \", val_loss ,\" and validation accuracy is \", val_acc)\n",
        "    \n",
        "    print(\"Best validation accuracy obtained is \", best_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataloader, defining model, calling train function, validation function for CombinedModelM3"
      ],
      "metadata": {
        "id": "_wEoq4Y_uJDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount = True)"
      ],
      "metadata": {
        "id": "7fERWcLUv3YI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11d4a6cb-8794-4364-dd17-2e6281b0622c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df2 = pd.read_csv('/content/drive/MyDrive/Inter IIT/finalm3_dataframe.csv')\n",
        "df2"
      ],
      "metadata": {
        "id": "Pd6nxXDiuDta",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "885972b3-6e91-4546-d338-6873e2c02b66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             Path_face  \\\n",
              "0                  /content/all_faces_peta/231_0_1.jpg   \n",
              "1                 /content/all_faces_peta/348_45_1.jpg   \n",
              "2                  /content/all_faces_peta/241_0_1.jpg   \n",
              "3                  /content/all_faces_peta/503_0_1.jpg   \n",
              "4                  /content/all_faces_peta/557_0_1.jpg   \n",
              "...                                                ...   \n",
              "18717   /content/all_faces_peta/203_4_FRAME_29_RGB.jpg   \n",
              "18718  /content/all_faces_peta/203_9_FRAME_103_RGB.jpg   \n",
              "18719  /content/all_faces_peta/203_9_FRAME_187_RGB.jpg   \n",
              "18720  /content/all_faces_peta/146_3_FRAME_102_RGB.jpg   \n",
              "18721  /content/all_faces_peta/146_3_FRAME_129_RGB.jpg   \n",
              "\n",
              "                                                    Path  Age  Gender  \n",
              "0        PETA dataset/PETA_jpg/VIPeR/archive/231_0_1.jpg    1       1  \n",
              "1       PETA dataset/PETA_jpg/VIPeR/archive/348_45_1.jpg    1       0  \n",
              "2        PETA dataset/PETA_jpg/VIPeR/archive/241_0_1.jpg    1       1  \n",
              "3        PETA dataset/PETA_jpg/VIPeR/archive/503_0_1.jpg    1       0  \n",
              "4        PETA dataset/PETA_jpg/VIPeR/archive/557_0_1.jpg    1       0  \n",
              "...                                                  ...  ...     ...  \n",
              "18717  PETA dataset/PETA_jpg/3DPeS/archive/203_4_FRAM...    1       1  \n",
              "18718  PETA dataset/PETA_jpg/3DPeS/archive/203_9_FRAM...    1       1  \n",
              "18719  PETA dataset/PETA_jpg/3DPeS/archive/203_9_FRAM...    1       1  \n",
              "18720  PETA dataset/PETA_jpg/3DPeS/archive/146_3_FRAM...    3       1  \n",
              "18721  PETA dataset/PETA_jpg/3DPeS/archive/146_3_FRAM...    3       1  \n",
              "\n",
              "[18722 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-be079403-9580-47d2-9dcb-cf5e333d58c3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Path_face</th>\n",
              "      <th>Path</th>\n",
              "      <th>Age</th>\n",
              "      <th>Gender</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/all_faces_peta/231_0_1.jpg</td>\n",
              "      <td>PETA dataset/PETA_jpg/VIPeR/archive/231_0_1.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/all_faces_peta/348_45_1.jpg</td>\n",
              "      <td>PETA dataset/PETA_jpg/VIPeR/archive/348_45_1.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/all_faces_peta/241_0_1.jpg</td>\n",
              "      <td>PETA dataset/PETA_jpg/VIPeR/archive/241_0_1.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/all_faces_peta/503_0_1.jpg</td>\n",
              "      <td>PETA dataset/PETA_jpg/VIPeR/archive/503_0_1.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/all_faces_peta/557_0_1.jpg</td>\n",
              "      <td>PETA dataset/PETA_jpg/VIPeR/archive/557_0_1.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18717</th>\n",
              "      <td>/content/all_faces_peta/203_4_FRAME_29_RGB.jpg</td>\n",
              "      <td>PETA dataset/PETA_jpg/3DPeS/archive/203_4_FRAM...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18718</th>\n",
              "      <td>/content/all_faces_peta/203_9_FRAME_103_RGB.jpg</td>\n",
              "      <td>PETA dataset/PETA_jpg/3DPeS/archive/203_9_FRAM...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18719</th>\n",
              "      <td>/content/all_faces_peta/203_9_FRAME_187_RGB.jpg</td>\n",
              "      <td>PETA dataset/PETA_jpg/3DPeS/archive/203_9_FRAM...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18720</th>\n",
              "      <td>/content/all_faces_peta/146_3_FRAME_102_RGB.jpg</td>\n",
              "      <td>PETA dataset/PETA_jpg/3DPeS/archive/146_3_FRAM...</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18721</th>\n",
              "      <td>/content/all_faces_peta/146_3_FRAME_129_RGB.jpg</td>\n",
              "      <td>PETA dataset/PETA_jpg/3DPeS/archive/146_3_FRAM...</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>18722 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-be079403-9580-47d2-9dcb-cf5e333d58c3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-be079403-9580-47d2-9dcb-cf5e333d58c3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-be079403-9580-47d2-9dcb-cf5e333d58c3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/drive/MyDrive/Inter IIT/testing files/all_faces1.zip' -d '/content/all_peta_faces2'\n",
        "!mv '/content/all_peta_faces2/content/all_faces_peta' '/content/all_faces_peta'"
      ],
      "metadata": {
        "id": "V4SQUBJdyuBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split as tts"
      ],
      "metadata": {
        "id": "Pzwg5JxpTYxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/all_faces_peta.zip /content/all_faces_peta\n",
        "from google.colab import files\n",
        "files.download(\"/content/all_faces_peta.zip\")"
      ],
      "metadata": {
        "id": "I3V8mGI4ouHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split as tts\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from tqdm.auto import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "ZpmqgkU1y3bx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def func(row):\n",
        "  return os.path.exists(row['Path_face']) & os.path.exists('/content/drive/MyDrive/'+row['Path'])"
      ],
      "metadata": {
        "id": "Q5BYnQ--8nfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = df2.drop(list(df2[df2['Gender']==2].index))\n",
        "df2 = df2.drop(list(df2[df2['Age']==5].index))\n",
        "\n",
        "## Removing the non-existent images\n",
        "df2['exist']  = df2.apply(func,axis = 1)\n",
        "df2 = df2[df2['exist']==True]\n",
        "df2 = df2.drop(columns = ['exist'], axis = 1)"
      ],
      "metadata": {
        "id": "uxtxFiIs1VjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Splitting the dataframe, into train,val and test\n",
        "\n",
        "from sklearn.model_selection import train_test_split as tts\n",
        "train_df, valid_df = tts(df2, test_size= 0.3)\n",
        "valid_df, test_df = tts(valid_df, test_size = 0.2)"
      ],
      "metadata": {
        "id": "BZfymzzoANz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Train_Prepare(Dataset):\n",
        "  \"\"\"\n",
        "  The Class will act as the container for our dataset. It will take your dataframe, the root path, and also the transform function for transforming the dataset.\n",
        "  \"\"\"\n",
        "  def __init__(self, data_frame, root_dir_face, root_dir_body, transform_face=None, transform_body=None):\n",
        "    self.data_frame = data_frame\n",
        "    self.root_dir_face = root_dir_face\n",
        "    self.root_dir_body = root_dir_body\n",
        "    self.transform_face = transform_face\n",
        "    self.transform_body = transform_body\n",
        "\n",
        "  def __len__(self):\n",
        "    # Return the length of the dataset\n",
        "    return len(self.data_frame)\n",
        "    \n",
        "  def __getitem__(self, idx):\n",
        "    # Return the observation based on an index. Ex. dataset[0] will return the first element from the dataset, in this case the image and the label.\n",
        "    if torch.is_tensor(idx):\n",
        "      idx = idx.tolist()    \n",
        "\n",
        "    try:\n",
        "      img_name_face = os.path.join(self.root_dir_face, str(self.data_frame.iloc[idx, 0]))\n",
        "      image_face = Image.open(img_name_face)\n",
        "\n",
        "    except:\n",
        "      image_face = np.zeros(1000,1000, 3)\n",
        "    try:    \n",
        "      img_name_body = os.path.join(self.root_dir_body, str(self.data_frame.iloc[idx, 1]))\n",
        "      image_body = Image.open(img_name_body)\n",
        "    \n",
        "    except:\n",
        "      image_body = np.zeros(1000,1000,3)\n",
        "    \n",
        "    label_age = self.data_frame.iloc[idx, 2]\n",
        "    label_gender = self.data_frame.iloc[idx, 3]\n",
        "    if self.transform_face:\n",
        "      image_face = self.transform_face(image_face)\n",
        "    if self.transform_body:\n",
        "      image_body = self.transform_body(image_body)\n",
        "    return (image_face, image_body, label_age, label_gender)\n",
        "\n",
        "class Val_Prepare(Dataset):\n",
        "  \"\"\"\n",
        "  The Class will act as the container for our dataset. It will take your dataframe, the root path, and also the transform function for transforming the dataset.\n",
        "  \"\"\"\n",
        "  def __init__(self, data_frame, root_dir_face, root_dir_body, transform_face=None, transform_body=None):\n",
        "    self.data_frame = data_frame\n",
        "    self.root_dir_face = root_dir_face\n",
        "    self.root_dir_body = root_dir_body\n",
        "    self.transform_face = transform_face\n",
        "    self.transform_body = transform_body\n",
        "  def __len__(self):\n",
        "    # Return the length of the dataset\n",
        "    return len(self.data_frame)\n",
        "  def __getitem__(self, idx):\n",
        "    # Return the observation based on an index. Ex. dataset[0] will return the first element from the dataset, in this case the image and the label.\n",
        "    if torch.is_tensor(idx):\n",
        "      idx = idx.tolist()    \n",
        "    try:\n",
        "      img_name_face = os.path.join(self.root_dir_face, str(self.data_frame.iloc[idx, 0]))\n",
        "      image_face = Image.open(img_name_face)\n",
        "\n",
        "    except:\n",
        "      image_face = np.zeros(1000,1000, 3)\n",
        "    try:    \n",
        "      img_name_body = os.path.join(self.root_dir_body, str(self.data_frame.iloc[idx, 1]))\n",
        "      image_body = Image.open(img_name_body)\n",
        "    \n",
        "    except:\n",
        "      image_body = np.zeros(1000,1000,3)\n",
        "    label_age = self.data_frame.iloc[idx, 2]\n",
        "    label_gender = self.data_frame.iloc[idx, 3]\n",
        "    if self.transform_face:\n",
        "      image_face = self.transform_face(image_face)\n",
        "    if self.transform_body:\n",
        "      image_body = self.transform_body(image_body)\n",
        "    return (image_face, image_body, label_age, label_gender)\n",
        "\n",
        "def data_preparation(Data_Class_1, Data_Class_2, root_directory_face, root_directory_body, train_df, val_df, Batch_Size = 128, Shuffle = False):\n",
        "  train_dataset = Data_Class_1(data_frame=train_df,root_dir_face=root_directory_face, root_dir_body=root_directory_body, transform_face = transforms.Compose([transforms.ToTensor(), transforms.Resize((100, 100))]), transform_body = transforms.Compose([transforms.ToTensor(), transforms.Resize((224, 100))]))\n",
        "  train_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = Batch_Size, shuffle = Shuffle, pin_memory=True)\n",
        "  val_dataset = Data_Class_2(data_frame=val_df,root_dir_face=root_directory_face, root_dir_body = root_directory_body, transform_face = transforms.Compose([transforms.ToTensor(), transforms.Resize((100, 100))]), transform_body = transforms.Compose([transforms.ToTensor(), transforms.Resize((224, 100))]))\n",
        "  val_loader = torch.utils.data.DataLoader(dataset = val_dataset, batch_size = Batch_Size, shuffle = Shuffle, pin_memory=True)\n",
        "  return train_loader, val_loader"
      ],
      "metadata": {
        "id": "nW0ikEsEuISa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## train loader"
      ],
      "metadata": {
        "id": "X1usjOnJZLns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader, val_loader = data_preparation(Train_Prepare, Val_Prepare, '', '/content/drive/MyDrive/', train_df, valid_df, Batch_Size = 128, Shuffle = True)"
      ],
      "metadata": {
        "id": "B7qfxRA11Tx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Running the model\n",
        "run(train_loader, val_loader, 5)"
      ],
      "metadata": {
        "id": "M-NJf0rMUF86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "tZCN8xrRbMoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference of CombinedModelM3"
      ],
      "metadata": {
        "id": "6RRK-mAquCbZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ## Load the weights of the model, and rememer to remove the last classification layer from each of the model\\\n",
        "\n",
        "# '''\n",
        "# code for loadfing t\n",
        "# '''\n",
        "# !gdown\n",
        "\n",
        "model_combined = CombinedModelM3(vgg16, model_body)\n",
        "try:\n",
        "  model_combined.load_state_dict(torch.load('/content/drive/MyDrive/Inter IIT/0.162567.pth', map_location=torch.device('cpu')))\n",
        "except:\n",
        "  print(\"Cant load the weights\")"
      ],
      "metadata": {
        "id": "oScwUG5HxsRr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9b82c46-cbe8-4c2d-87fc-b93f39ebd5d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cant load the weights\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Test_Prepare_for_combined(Dataset):\n",
        "  \"\"\"\n",
        "  The Class will act as the container for our dataset. It will take your dataframe, the root path, and also the transform function for transforming the dataset.\n",
        "  \"\"\"\n",
        "  def __init__(self, data_frame, root_dir_face, root_dir_body, transform_face=None, transform_body=None):\n",
        "    self.data_frame = data_frame\n",
        "    self.root_dir_face = root_dir_face\n",
        "    self.root_dir_body = root_dir_body\n",
        "    self.transform_face = transform_face\n",
        "    self.transform_body = transform_body\n",
        "  def __len__(self):\n",
        "    # Return the length of the dataset\n",
        "    return len(self.data_frame)\n",
        "  def __getitem__(self, idx):\n",
        "    # Return the observation based on an index. Ex. dataset[0] will return the first element from the dataset, in this case the image and the label.\n",
        "    if torch.is_tensor(idx):\n",
        "      idx = idx.tolist()    \n",
        "    try:\n",
        "      img_name_face = os.path.join(self.root_dir_face, str(self.data_frame.iloc[idx, 0]))\n",
        "      image_face = Image.open(img_name_face)\n",
        "\n",
        "    except:\n",
        "      image_face = np.zeros(1000,1000, 3)\n",
        "    try:    \n",
        "      img_name_body = os.path.join(self.root_dir_body, str(self.data_frame.iloc[idx, 1]))\n",
        "      image_body = Image.open(img_name_body)\n",
        "    \n",
        "    except:\n",
        "      image_body = np.zeros(1000,1000,3)\n",
        "    if self.transform_face:\n",
        "      image_face = self.transform_face(image_face)\n",
        "    if self.transform_body:\n",
        "      image_body = self.transform_body(image_body)\n",
        "    return (image_face, image_body)\n"
      ],
      "metadata": {
        "id": "dlhkYK9thBeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ThFZXz3Inc7",
        "outputId": "45d872e8-baac-4e54-a4cd-157ee9fe75da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def data_preparation_test(Data_Class_1, root_directory_face, root_directory_body, train_df, Batch_Size = 128, Shuffle = False):\n",
        "  train_dataset = Data_Class_1(data_frame=train_df,root_dir_face=root_directory_face, root_dir_body=root_directory_body, transform_face = transforms.Compose([transforms.ToTensor(), transforms.Resize((100, 100))]), transform_body = transforms.Compose([transforms.ToTensor(), transforms.Resize((224, 100))]))\n",
        "  train_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = Batch_Size, shuffle = Shuffle, pin_memory=True)\n",
        "  return train_loader"
      ],
      "metadata": {
        "id": "qJS5gFGz5COz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader_peta = data_preparation_test(Test_Prepare_for_combined, '', '/content/drive/MyDrive/', test_df, Batch_Size = 128, Shuffle = True)"
      ],
      "metadata": {
        "id": "3QhR3DEmGqQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# out = next(iter(test_loader_peta))\n",
        "# len(out)"
      ],
      "metadata": {
        "id": "Hi-pbk3VMy30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Test(model, dataloaders, device, num_epochs=1):\n",
        "  model.eval()\n",
        "  model = model.to(device)\n",
        "  with torch.no_grad():\n",
        "\n",
        "      running_loss = 0.0\n",
        "      running_corrects = 0\n",
        "      age_p =[]\n",
        "      gender_p = []\n",
        "      for inputs_face, inputs_body in tqdm(dataloaders, total = len(dataloaders)):\n",
        "          inputs_face = inputs_face.to(device)\n",
        "          inputs_body = inputs_body.to(device)\n",
        "\n",
        "          age, gender = model(inputs_face, inputs_body)\n",
        "          _, preds_age = torch.max(age, 1)\n",
        "          _, preds_gender = torch.max(gender, 1)\n",
        "          age_p.extend(age.detach().cpu().numpy().tolist())\n",
        "          gender_p.extend(gender.detach().cpu().numpy().tolist())\n",
        "\n",
        "\n",
        "  return age_p, gender_p"
      ],
      "metadata": {
        "id": "NZGZ9oG2HMlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# out = next(iter(test_loader_peta))\n",
        "# out_t = next(iter(train_loader))\n",
        "# out_t[0].shape, out_t[1].shape\n",
        "# out[0].shape, out[1].shape\n",
        "\n",
        "# len(test_loader_peta)\n",
        "# out = next(iter(test_loader_peta))a"
      ],
      "metadata": {
        "id": "f7i0lpr6KV2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "age_p, gender_p = Test(model_combined, test_loader_peta, device = 'cuda')"
      ],
      "metadata": {
        "id": "IM5UR8_1IdNA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "d6fdc53c3d784ca594d1f576852a4621",
            "da1a393832a14e6f9d278a8704484829",
            "059aa9f0297a4166896098cdb186baa8",
            "b50b423c461944f78618242abb420ba9",
            "150d2971964641d1a3e74887939eb68b",
            "99185d49b04a4bf6afbbedbd2c30f465",
            "1d08889c429445c5987753596801bd2c",
            "cffc75f0c3e84794825940a4defaff55",
            "0eeaa194e0944ae6bc9da816b5f60d1a",
            "5816ba1a284744668d5811d82a73d27e",
            "e14d97996b9948b4b20d4e8d258a3032"
          ]
        },
        "outputId": "03b68b8d-bf26-4c8a-8984-5e6b3e1acada"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/6 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d6fdc53c3d784ca594d1f576852a4621"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_gender(x):\n",
        "  if x[0]>x[1]:\n",
        "      return 'M'\n",
        "  return 'F'"
      ],
      "metadata": {
        "id": "oefqHYXLPI6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gender_p_cat = list(map(lambda x: convert_to_gender(x),gender_p))"
      ],
      "metadata": {
        "id": "XbQ0TVC-O3M_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df['Gender'] = gender_p_cat"
      ],
      "metadata": {
        "id": "coPzKXPBIgNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "age_p_actual = np.argmax(np.array(age_p), axis = -1)"
      ],
      "metadata": {
        "id": "1YGqKxQGQTFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actual_age = []\n",
        "for i in age_p_actual:\n",
        "  entry = i\n",
        "  if entry == 0:\n",
        "    actual_age.append([1, 15])\n",
        "\n",
        "  elif entry == 1:\n",
        "    actual_age.append([16, 30])\n",
        "\n",
        "\n",
        "  elif entry == 2:\n",
        "    actual_age.append([31, 45])\n",
        "\n",
        "\n",
        "  elif entry == 3:\n",
        "    actual_age.append([46, 60])\n",
        "\n",
        "  else:\n",
        "    actual_age.append([60, 100])"
      ],
      "metadata": {
        "id": "AHzx4MG8I9Qr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actual_age = np.array(actual_age)"
      ],
      "metadata": {
        "id": "TpIewJm4JpPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if 'Age' in test_df.columns:\n",
        "  test_df.drop(columns = ['Age'] , axis = 1, inplace = True)"
      ],
      "metadata": {
        "id": "rixEtEw7Qq1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df['age_min'] = actual_age[:, 0]\n",
        "test_df['age_max'] = actual_age[:, 1]"
      ],
      "metadata": {
        "id": "Vyw3mnaWJmwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "ATSJb-GJQm-j",
        "outputId": "134f214d-ad3a-423b-e04c-06caca6989d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                  Path_face  \\\n",
              "2909   /content/all_faces_peta/172_3738.jpg   \n",
              "17084  /content/all_faces_peta/0055_019.jpg   \n",
              "17436  /content/all_faces_peta/0060_017.jpg   \n",
              "2289   /content/all_faces_peta/162_3390.jpg   \n",
              "4446   /content/all_faces_peta/108_2014.jpg   \n",
              "...                                     ...   \n",
              "1826   /content/all_faces_peta/215_4392.jpg   \n",
              "7779     /content/all_faces_peta/35_571.jpg   \n",
              "6345     /content/all_faces_peta/45_922.jpg   \n",
              "10295  /content/all_faces_peta/0071_002.jpg   \n",
              "10576  /content/all_faces_peta/0032_002.jpg   \n",
              "\n",
              "                                                    Path Gender  age_min  \\\n",
              "2909   PETA dataset/PETA_jpg/TownCentre/archive/172_3...      M       16   \n",
              "17084  PETA dataset/PETA_jpg/CAVIAR4REID/archive/0055...      M       16   \n",
              "17436  PETA dataset/PETA_jpg/CAVIAR4REID/archive/0060...      M       16   \n",
              "2289   PETA dataset/PETA_jpg/TownCentre/archive/162_3...      M       16   \n",
              "4446   PETA dataset/PETA_jpg/TownCentre/archive/108_2...      M       31   \n",
              "...                                                  ...    ...      ...   \n",
              "1826   PETA dataset/PETA_jpg/TownCentre/archive/215_4...      M       16   \n",
              "7779   PETA dataset/PETA_jpg/TownCentre/archive/35_57...      F       16   \n",
              "6345   PETA dataset/PETA_jpg/TownCentre/archive/45_92...      M       16   \n",
              "10295   PETA dataset/PETA_jpg/i-LID/archive/0071_002.jpg      M       31   \n",
              "10576   PETA dataset/PETA_jpg/i-LID/archive/0032_002.jpg      M       16   \n",
              "\n",
              "       age_max  \n",
              "2909        30  \n",
              "17084       30  \n",
              "17436       30  \n",
              "2289        30  \n",
              "4446        45  \n",
              "...        ...  \n",
              "1826        30  \n",
              "7779        30  \n",
              "6345        30  \n",
              "10295       45  \n",
              "10576       30  \n",
              "\n",
              "[651 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a5a42545-7342-495a-bfe5-cd0e080069b8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Path_face</th>\n",
              "      <th>Path</th>\n",
              "      <th>Gender</th>\n",
              "      <th>age_min</th>\n",
              "      <th>age_max</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2909</th>\n",
              "      <td>/content/all_faces_peta/172_3738.jpg</td>\n",
              "      <td>PETA dataset/PETA_jpg/TownCentre/archive/172_3...</td>\n",
              "      <td>M</td>\n",
              "      <td>16</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17084</th>\n",
              "      <td>/content/all_faces_peta/0055_019.jpg</td>\n",
              "      <td>PETA dataset/PETA_jpg/CAVIAR4REID/archive/0055...</td>\n",
              "      <td>M</td>\n",
              "      <td>16</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17436</th>\n",
              "      <td>/content/all_faces_peta/0060_017.jpg</td>\n",
              "      <td>PETA dataset/PETA_jpg/CAVIAR4REID/archive/0060...</td>\n",
              "      <td>M</td>\n",
              "      <td>16</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2289</th>\n",
              "      <td>/content/all_faces_peta/162_3390.jpg</td>\n",
              "      <td>PETA dataset/PETA_jpg/TownCentre/archive/162_3...</td>\n",
              "      <td>M</td>\n",
              "      <td>16</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4446</th>\n",
              "      <td>/content/all_faces_peta/108_2014.jpg</td>\n",
              "      <td>PETA dataset/PETA_jpg/TownCentre/archive/108_2...</td>\n",
              "      <td>M</td>\n",
              "      <td>31</td>\n",
              "      <td>45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1826</th>\n",
              "      <td>/content/all_faces_peta/215_4392.jpg</td>\n",
              "      <td>PETA dataset/PETA_jpg/TownCentre/archive/215_4...</td>\n",
              "      <td>M</td>\n",
              "      <td>16</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7779</th>\n",
              "      <td>/content/all_faces_peta/35_571.jpg</td>\n",
              "      <td>PETA dataset/PETA_jpg/TownCentre/archive/35_57...</td>\n",
              "      <td>F</td>\n",
              "      <td>16</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6345</th>\n",
              "      <td>/content/all_faces_peta/45_922.jpg</td>\n",
              "      <td>PETA dataset/PETA_jpg/TownCentre/archive/45_92...</td>\n",
              "      <td>M</td>\n",
              "      <td>16</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10295</th>\n",
              "      <td>/content/all_faces_peta/0071_002.jpg</td>\n",
              "      <td>PETA dataset/PETA_jpg/i-LID/archive/0071_002.jpg</td>\n",
              "      <td>M</td>\n",
              "      <td>31</td>\n",
              "      <td>45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10576</th>\n",
              "      <td>/content/all_faces_peta/0032_002.jpg</td>\n",
              "      <td>PETA dataset/PETA_jpg/i-LID/archive/0032_002.jpg</td>\n",
              "      <td>M</td>\n",
              "      <td>16</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>651 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a5a42545-7342-495a-bfe5-cd0e080069b8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a5a42545-7342-495a-bfe5-cd0e080069b8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a5a42545-7342-495a-bfe5-cd0e080069b8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 218
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test_loader = data_preparation(Test_Prepare_for_combined, '/content/face_images/', '/content/body_images', test_df, Batch_Size = 128, Shuffle = True)"
      ],
      "metadata": {
        "id": "Sz4PGzT2GXLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def func_get_box(row, all_boxes, parameter = 'xmin'):\n",
        "  if parameter == 'xmin':\n",
        "    return all_boxes[row['Path']][0]\n",
        "  elif parameter == 'ymin':\n",
        "    return all_boxes[row['Path']][2]\n",
        "  elif parameter == 'width':\n",
        "    return all_boxes[row['Path']][1] -  all_boxes[row['Path']][0]\n",
        "  else:\n",
        "    return all_boxes[row['Path']][3] -  all_boxes[row['Path']][2]\n"
      ],
      "metadata": {
        "id": "9-L5IPdvNt9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_boxes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvBbjpIJPsaN",
        "outputId": "d4bfbb0e-906b-40e9-fe44-65217f47e497"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'m1_cropped/B_0_0_BC0.jpg': [481.35617, 301.11188, 674.0922, 893.75665],\n",
              " 'm1_cropped/B_0_0_BC1.jpg': [135.76279, 303.2284, 287.5884, 798.2684]}"
            ]
          },
          "metadata": {},
          "execution_count": 199
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "n9iWLrOrRFOP",
        "outputId": "429e0f50-0102-447d-afdb-45821fbab2d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                  Path_face  \\\n",
              "2909   /content/all_faces_peta/172_3738.jpg   \n",
              "17084  /content/all_faces_peta/0055_019.jpg   \n",
              "17436  /content/all_faces_peta/0060_017.jpg   \n",
              "2289   /content/all_faces_peta/162_3390.jpg   \n",
              "4446   /content/all_faces_peta/108_2014.jpg   \n",
              "...                                     ...   \n",
              "1826   /content/all_faces_peta/215_4392.jpg   \n",
              "7779     /content/all_faces_peta/35_571.jpg   \n",
              "6345     /content/all_faces_peta/45_922.jpg   \n",
              "10295  /content/all_faces_peta/0071_002.jpg   \n",
              "10576  /content/all_faces_peta/0032_002.jpg   \n",
              "\n",
              "                                                    Path Gender  age_min  \\\n",
              "2909   PETA dataset/PETA_jpg/TownCentre/archive/172_3...      M       16   \n",
              "17084  PETA dataset/PETA_jpg/CAVIAR4REID/archive/0055...      M       16   \n",
              "17436  PETA dataset/PETA_jpg/CAVIAR4REID/archive/0060...      M       16   \n",
              "2289   PETA dataset/PETA_jpg/TownCentre/archive/162_3...      M       16   \n",
              "4446   PETA dataset/PETA_jpg/TownCentre/archive/108_2...      M       31   \n",
              "...                                                  ...    ...      ...   \n",
              "1826   PETA dataset/PETA_jpg/TownCentre/archive/215_4...      M       16   \n",
              "7779   PETA dataset/PETA_jpg/TownCentre/archive/35_57...      F       16   \n",
              "6345   PETA dataset/PETA_jpg/TownCentre/archive/45_92...      M       16   \n",
              "10295   PETA dataset/PETA_jpg/i-LID/archive/0071_002.jpg      M       31   \n",
              "10576   PETA dataset/PETA_jpg/i-LID/archive/0032_002.jpg      M       16   \n",
              "\n",
              "       age_max  \n",
              "2909        30  \n",
              "17084       30  \n",
              "17436       30  \n",
              "2289        30  \n",
              "4446        45  \n",
              "...        ...  \n",
              "1826        30  \n",
              "7779        30  \n",
              "6345        30  \n",
              "10295       45  \n",
              "10576       30  \n",
              "\n",
              "[651 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-40f61c7c-8544-4095-9a93-9c30db28e97a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Path_face</th>\n",
              "      <th>Path</th>\n",
              "      <th>Gender</th>\n",
              "      <th>age_min</th>\n",
              "      <th>age_max</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2909</th>\n",
              "      <td>/content/all_faces_peta/172_3738.jpg</td>\n",
              "      <td>PETA dataset/PETA_jpg/TownCentre/archive/172_3...</td>\n",
              "      <td>M</td>\n",
              "      <td>16</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17084</th>\n",
              "      <td>/content/all_faces_peta/0055_019.jpg</td>\n",
              "      <td>PETA dataset/PETA_jpg/CAVIAR4REID/archive/0055...</td>\n",
              "      <td>M</td>\n",
              "      <td>16</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17436</th>\n",
              "      <td>/content/all_faces_peta/0060_017.jpg</td>\n",
              "      <td>PETA dataset/PETA_jpg/CAVIAR4REID/archive/0060...</td>\n",
              "      <td>M</td>\n",
              "      <td>16</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2289</th>\n",
              "      <td>/content/all_faces_peta/162_3390.jpg</td>\n",
              "      <td>PETA dataset/PETA_jpg/TownCentre/archive/162_3...</td>\n",
              "      <td>M</td>\n",
              "      <td>16</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4446</th>\n",
              "      <td>/content/all_faces_peta/108_2014.jpg</td>\n",
              "      <td>PETA dataset/PETA_jpg/TownCentre/archive/108_2...</td>\n",
              "      <td>M</td>\n",
              "      <td>31</td>\n",
              "      <td>45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1826</th>\n",
              "      <td>/content/all_faces_peta/215_4392.jpg</td>\n",
              "      <td>PETA dataset/PETA_jpg/TownCentre/archive/215_4...</td>\n",
              "      <td>M</td>\n",
              "      <td>16</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7779</th>\n",
              "      <td>/content/all_faces_peta/35_571.jpg</td>\n",
              "      <td>PETA dataset/PETA_jpg/TownCentre/archive/35_57...</td>\n",
              "      <td>F</td>\n",
              "      <td>16</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6345</th>\n",
              "      <td>/content/all_faces_peta/45_922.jpg</td>\n",
              "      <td>PETA dataset/PETA_jpg/TownCentre/archive/45_92...</td>\n",
              "      <td>M</td>\n",
              "      <td>16</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10295</th>\n",
              "      <td>/content/all_faces_peta/0071_002.jpg</td>\n",
              "      <td>PETA dataset/PETA_jpg/i-LID/archive/0071_002.jpg</td>\n",
              "      <td>M</td>\n",
              "      <td>31</td>\n",
              "      <td>45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10576</th>\n",
              "      <td>/content/all_faces_peta/0032_002.jpg</td>\n",
              "      <td>PETA dataset/PETA_jpg/i-LID/archive/0032_002.jpg</td>\n",
              "      <td>M</td>\n",
              "      <td>16</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>651 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-40f61c7c-8544-4095-9a93-9c30db28e97a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-40f61c7c-8544-4095-9a93-9c30db28e97a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-40f61c7c-8544-4095-9a93-9c30db28e97a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 219
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_df['bb_xmin'] = test_df.apply(lambda x: func_get_box(x, all_boxes, 'xmin'),axis = 1)\n",
        "test_df['bb_ymin'] = test_df.apply(lambda x: func_get_box(x, all_boxes, 'ymin'),axis = 1)\n",
        "test_df['bb_width'] = test_df.apply(lambda x: func_get_box(x, all_boxes, 'width'),axis = 1)\n",
        "test_df['bb_height'] = test_df.apply(lambda x: func_get_box(x, all_boxes, 'height'),axis = 1)"
      ],
      "metadata": {
        "id": "c_nWpdslNt3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MYvDW1uQSOjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.to_csv('submission.csv', index = False)"
      ],
      "metadata": {
        "id": "6kZbsQQUIsRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "print(\"Gender Accuracy:\", accuracy_score(test_df['Gender'], test_df['gender']))"
      ],
      "metadata": {
        "id": "0oxf0cjdNtzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count=0\n",
        "for i in range(len(test_df)):\n",
        "  if(test_df['Age'][i]>test_df['Age'][i] and test_df['Age'][i]<test_df['Age'][i]):\n",
        "    count+=1\n",
        "print(\"Gender Accuracy:\", (count/len(test_df)))"
      ],
      "metadata": {
        "id": "Ma4utj33NtxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tXSxXrU_Tdyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Making a final submission"
      ],
      "metadata": {
        "id": "DU1NhBY7TeZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "age = list(all_boxes.values())"
      ],
      "metadata": {
        "id": "O1S1BdVbT1IG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xmin = list(map(lambda x: x[0], age))\n",
        "ymin = list(map(lambda x: x[1], age))\n",
        "xmax = list(map(lambda x: x[2], age))\n",
        "ymax = list(map(lambda x: x[3], age))\n",
        "final_df = pd.DataFrame({'Image Path': all_boxes.keys(), 'xmin': xmin, 'ymin': ymin, 'xmax': xmax, 'ymax': ymax})"
      ],
      "metadata": {
        "id": "XJyHmIx4TdtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Test_Prepare(Dataset):\n",
        "  \"\"\"\n",
        "  The Class will act as the container for our dataset. It will take your dataframe, the root path, and also the transform function for transforming the dataset.\n",
        "  \"\"\"\n",
        "  def __init__(self, entry_body, entry_face, transform_face=None, transform_body=None):\n",
        "    self.entry_body = sorted(entry_body)\n",
        "    self.entry_face = sorted(entry_face)\n",
        "    self.transform_face = transform_face\n",
        "    self.transform_body = transform_body\n",
        "  def __len__(self):\n",
        "    # Return the length of the dataset\n",
        "    return len(self.entry_body)\n",
        "  def __getitem__(self, idx):\n",
        "    # Return the observation based on an index. Ex. dataset[0] will return the first element from the dataset, in this case the image and the label.\n",
        "    if torch.is_tensor(idx):\n",
        "      idx = idx.tolist()    \n",
        "    try:\n",
        "      img_name_face =str(self.entry_face[idx])\n",
        "      image_face = Image.open(img_name_face)\n",
        "\n",
        "    except:\n",
        "      image_face = np.zeros((1000,1000, 3))\n",
        "    try:    \n",
        "      img_name_body = str(self.entry_body[idx])\n",
        "      image_body = Image.open(img_name_body)\n",
        "    \n",
        "    except:\n",
        "      image_body = np.zeros((1000,1000,3))\n",
        "\n",
        "    if self.transform_face:\n",
        "      image_face = self.transform_face(image_face)\n",
        "    if self.transform_body:\n",
        "      image_body = self.transform_body(image_body)\n",
        "    return image_face.float(), image_body.float(), img_name_body"
      ],
      "metadata": {
        "id": "N63OFbu6UiDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "face_entry = ['/content/face_images/'+ i for i in os.listdir('/content/face_images')]\n",
        "body_entry = ['/content/m1_cropped/'+ i for i in os.listdir('/content/m1_cropped')]"
      ],
      "metadata": {
        "id": "a8dU17mvV3TM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = Test_Prepare(body_entry,face_entry, transform_face = transforms.Compose([transforms.ToTensor(), transforms.Resize((100, 100))]), transform_body = transforms.Compose([transforms.ToTensor(), transforms.Resize((224, 100))]))"
      ],
      "metadata": {
        "id": "kD6ceEpVWSKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dl = DataLoader(test_dataset, batch_size = 1, shuffle = False)"
      ],
      "metadata": {
        "id": "KLGYHTb3WqwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Test(model, dataloaders, device, num_epochs=1):\n",
        "  model.eval()\n",
        "  model = model.to(device)\n",
        "  with torch.no_grad():\n",
        "\n",
        "      running_loss = 0.0\n",
        "      running_corrects = 0\n",
        "      age_p =[]\n",
        "      names = []\n",
        "      gender_p = []\n",
        "      for inputs_face, inputs_body, name in tqdm(dataloaders, total = len(dataloaders)):\n",
        "          inputs_face = inputs_face.to(device)\n",
        "          inputs_body = inputs_body.to(device)\n",
        "\n",
        "          age, gender = model(inputs_face, inputs_body)\n",
        "          _, preds_age = torch.max(age, 1)\n",
        "          _, preds_gender = torch.max(gender, 1)\n",
        "          age_p.extend(age.detach().cpu().numpy().tolist())\n",
        "          gender_p.extend(gender.detach().cpu().numpy().tolist())\n",
        "          names.extend(name)\n",
        "\n",
        "\n",
        "  return age_p, gender_p, names"
      ],
      "metadata": {
        "id": "TWICxZOZXuM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "age_p, gender_p, names =  Test(model_combined, test_dl, device = 'cuda')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "61fa2a3c51744515a1e50ac0dbe45359",
            "f8d46724258c4cc68506ff7b365fdfa9",
            "a645f9416c04446c848dc86a0dcb5e00",
            "0e554ad86f6e441dbb3c3c2324d41fdc",
            "f1214219919c494a9f7833a3aa2e021e",
            "1b585d9da89b4c269231a857875a0eac",
            "cb186b4617c94d638bae5a4a31ace50b",
            "7d963b02b8f94b4d951fd4ae12e871a8",
            "4b8593c75fbd460eae413c45db5bd41b",
            "24da82e1165940c9ae28fa6b4e3233c5",
            "795ec13256574b4484747c701c3f77f8"
          ]
        },
        "id": "OskRINrqTdrH",
        "outputId": "2b572838-c695-4f37-eb1b-c8337ed20428"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "61fa2a3c51744515a1e50ac0dbe45359"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gender_p_cat = list(map(lambda x: convert_to_gender(x),gender_p))"
      ],
      "metadata": {
        "id": "Bq_UfjRcYMRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "age_p_actual = np.argmax(np.array(age_p), axis = -1)"
      ],
      "metadata": {
        "id": "FJN-Q_cNYMRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actual_age = []\n",
        "for i in age_p_actual:\n",
        "  entry = i\n",
        "  if entry == 0:\n",
        "    actual_age.append([1, 15])\n",
        "\n",
        "  elif entry == 1:\n",
        "    actual_age.append([16, 30])\n",
        "\n",
        "\n",
        "  elif entry == 2:\n",
        "    actual_age.append([31, 45])\n",
        "\n",
        "\n",
        "  elif entry == 3:\n",
        "    actual_age.append([46, 60])\n",
        "\n",
        "  else:\n",
        "    actual_age.append([60, 100])"
      ],
      "metadata": {
        "id": "67FHn-nnYMRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actual_age = np.array(actual_age)"
      ],
      "metadata": {
        "id": "xJrMEg2PYMRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gender_p_cat = list(map(lambda x: convert_to_gender(x),gender_p))"
      ],
      "metadata": {
        "id": "YfxNPD9IZk2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_dict = {}\n",
        "gender = []\n",
        "age_min = []\n",
        "age_max = []\n",
        "for i, j, k in zip(names, gender_p_cat, actual_age):\n",
        "  gender.append(j)\n",
        "  age_min.append(k[0])\n",
        "  age_max.append(k[1])\n",
        "  gender.append(j)"
      ],
      "metadata": {
        "id": "OF6ypgAyYSim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gender_p_cat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjlJKW3uZetE",
        "outputId": "3349aa9b-3f86-464f-f6d6-b6294a2b7501"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['M', 'M']"
            ]
          },
          "metadata": {},
          "execution_count": 295
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_df['gender'] = gender_p_cat\n",
        "final_df['age_min'] = age_min\n",
        "final_df['age_max'] = age_max"
      ],
      "metadata": {
        "id": "rTsl2MK-ZR_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "7hzgca6BYVNO",
        "outputId": "032581d9-a1bd-445e-9d4d-009298f6dd9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 Image Path        xmin        ymin        xmax        ymax  \\\n",
              "0  m1_cropped/B_0_0_BC0.jpg  481.356171  301.111877  674.092224  893.756653   \n",
              "1  m1_cropped/B_0_0_BC1.jpg  135.762787  303.228394  287.588409  798.268372   \n",
              "\n",
              "  gender  age_min  age_max  \n",
              "0      M       16       30  \n",
              "1      M       16       30  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5797368f-a919-4f22-8959-138034a1d3b4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Image Path</th>\n",
              "      <th>xmin</th>\n",
              "      <th>ymin</th>\n",
              "      <th>xmax</th>\n",
              "      <th>ymax</th>\n",
              "      <th>gender</th>\n",
              "      <th>age_min</th>\n",
              "      <th>age_max</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>m1_cropped/B_0_0_BC0.jpg</td>\n",
              "      <td>481.356171</td>\n",
              "      <td>301.111877</td>\n",
              "      <td>674.092224</td>\n",
              "      <td>893.756653</td>\n",
              "      <td>M</td>\n",
              "      <td>16</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>m1_cropped/B_0_0_BC1.jpg</td>\n",
              "      <td>135.762787</td>\n",
              "      <td>303.228394</td>\n",
              "      <td>287.588409</td>\n",
              "      <td>798.268372</td>\n",
              "      <td>M</td>\n",
              "      <td>16</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5797368f-a919-4f22-8959-138034a1d3b4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5797368f-a919-4f22-8959-138034a1d3b4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5797368f-a919-4f22-8959-138034a1d3b4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 297
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.to_csv('submission.csv', index = False)"
      ],
      "metadata": {
        "id": "Jx3HPYiMYcCk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "BOSCH's age and Gender Prediction.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c7cf955f754b4a309a46f40f1784f030": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_495b195f78364a08895a149d2cfd06eb",
              "IPY_MODEL_a43e3e749b8949829c0813649fdf1a1c",
              "IPY_MODEL_95c80c32eb934a8b910cb76e4158c3ba"
            ],
            "layout": "IPY_MODEL_82dbc0dff7e340fab19e495f73373d8e"
          }
        },
        "495b195f78364a08895a149d2cfd06eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bba2717473564d679f6be144972a49a4",
            "placeholder": "​",
            "style": "IPY_MODEL_fed6d7567b8c45628b791cf09fc6df5f",
            "value": "100%"
          }
        },
        "a43e3e749b8949829c0813649fdf1a1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24fd2965fc564c169378e9f4fb829822",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bb4477cbc85444c69d07dfbb87db7951",
            "value": 1
          }
        },
        "95c80c32eb934a8b910cb76e4158c3ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_896db197c10f4036a783d1544ac7ba07",
            "placeholder": "​",
            "style": "IPY_MODEL_220f6cd7b91e4dcaa0b8716dc321caec",
            "value": " 1/1 [00:00&lt;00:00, 11.30it/s]"
          }
        },
        "82dbc0dff7e340fab19e495f73373d8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bba2717473564d679f6be144972a49a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fed6d7567b8c45628b791cf09fc6df5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "24fd2965fc564c169378e9f4fb829822": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb4477cbc85444c69d07dfbb87db7951": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "896db197c10f4036a783d1544ac7ba07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "220f6cd7b91e4dcaa0b8716dc321caec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d6fdc53c3d784ca594d1f576852a4621": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_da1a393832a14e6f9d278a8704484829",
              "IPY_MODEL_059aa9f0297a4166896098cdb186baa8",
              "IPY_MODEL_b50b423c461944f78618242abb420ba9"
            ],
            "layout": "IPY_MODEL_150d2971964641d1a3e74887939eb68b"
          }
        },
        "da1a393832a14e6f9d278a8704484829": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99185d49b04a4bf6afbbedbd2c30f465",
            "placeholder": "​",
            "style": "IPY_MODEL_1d08889c429445c5987753596801bd2c",
            "value": "100%"
          }
        },
        "059aa9f0297a4166896098cdb186baa8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cffc75f0c3e84794825940a4defaff55",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0eeaa194e0944ae6bc9da816b5f60d1a",
            "value": 6
          }
        },
        "b50b423c461944f78618242abb420ba9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5816ba1a284744668d5811d82a73d27e",
            "placeholder": "​",
            "style": "IPY_MODEL_e14d97996b9948b4b20d4e8d258a3032",
            "value": " 6/6 [01:25&lt;00:00, 11.47s/it]"
          }
        },
        "150d2971964641d1a3e74887939eb68b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99185d49b04a4bf6afbbedbd2c30f465": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d08889c429445c5987753596801bd2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cffc75f0c3e84794825940a4defaff55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0eeaa194e0944ae6bc9da816b5f60d1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5816ba1a284744668d5811d82a73d27e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e14d97996b9948b4b20d4e8d258a3032": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "61fa2a3c51744515a1e50ac0dbe45359": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f8d46724258c4cc68506ff7b365fdfa9",
              "IPY_MODEL_a645f9416c04446c848dc86a0dcb5e00",
              "IPY_MODEL_0e554ad86f6e441dbb3c3c2324d41fdc"
            ],
            "layout": "IPY_MODEL_f1214219919c494a9f7833a3aa2e021e"
          }
        },
        "f8d46724258c4cc68506ff7b365fdfa9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b585d9da89b4c269231a857875a0eac",
            "placeholder": "​",
            "style": "IPY_MODEL_cb186b4617c94d638bae5a4a31ace50b",
            "value": "100%"
          }
        },
        "a645f9416c04446c848dc86a0dcb5e00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d963b02b8f94b4d951fd4ae12e871a8",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4b8593c75fbd460eae413c45db5bd41b",
            "value": 2
          }
        },
        "0e554ad86f6e441dbb3c3c2324d41fdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24da82e1165940c9ae28fa6b4e3233c5",
            "placeholder": "​",
            "style": "IPY_MODEL_795ec13256574b4484747c701c3f77f8",
            "value": " 2/2 [00:00&lt;00:00,  8.91it/s]"
          }
        },
        "f1214219919c494a9f7833a3aa2e021e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b585d9da89b4c269231a857875a0eac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb186b4617c94d638bae5a4a31ace50b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7d963b02b8f94b4d951fd4ae12e871a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b8593c75fbd460eae413c45db5bd41b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "24da82e1165940c9ae28fa6b4e3233c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "795ec13256574b4484747c701c3f77f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aff15737fb004a728124e74a92bce2ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_31aac39166224797bd90dc46cb90db91",
              "IPY_MODEL_a33c3bc55a584aa399ef83c8cc238fa5",
              "IPY_MODEL_58eb380d12324bdfa431cf4492cc9f54"
            ],
            "layout": "IPY_MODEL_e860de30ea764e5898babd6cba5fedde"
          }
        },
        "31aac39166224797bd90dc46cb90db91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_888f2e5574a84de1aa583a3436d3f4bf",
            "placeholder": "​",
            "style": "IPY_MODEL_999079e7fa344d96b854afba59fc323f",
            "value": "100%"
          }
        },
        "a33c3bc55a584aa399ef83c8cc238fa5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19482c64aacb4bb6b7fd0f25578e2e25",
            "max": 553433881,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cce1b7fa6af14afba35e64ae70022bc2",
            "value": 553433881
          }
        },
        "58eb380d12324bdfa431cf4492cc9f54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44c91eb636544d2cb95d66c16feb1284",
            "placeholder": "​",
            "style": "IPY_MODEL_3830dafb2f5b494b856042cc8abf3428",
            "value": " 528M/528M [00:06&lt;00:00, 102MB/s]"
          }
        },
        "e860de30ea764e5898babd6cba5fedde": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "888f2e5574a84de1aa583a3436d3f4bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "999079e7fa344d96b854afba59fc323f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "19482c64aacb4bb6b7fd0f25578e2e25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cce1b7fa6af14afba35e64ae70022bc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "44c91eb636544d2cb95d66c16feb1284": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3830dafb2f5b494b856042cc8abf3428": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "62bd0fc0b29a4e18a3f11bafb7f1ce3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dd09ef986c0642b187cbe2259c51bf2c",
              "IPY_MODEL_3d9be96fcf0e496480f35dea60f0134d",
              "IPY_MODEL_7087bb1e3cec40a3a98d52ad17234194"
            ],
            "layout": "IPY_MODEL_b345a3d40e334fcd95029e638165926f"
          }
        },
        "dd09ef986c0642b187cbe2259c51bf2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be7e2ad889814fa1a471c674f354ce9e",
            "placeholder": "​",
            "style": "IPY_MODEL_a22ab99c0d3b4ea98e76cd7b412ec14f",
            "value": "100%"
          }
        },
        "3d9be96fcf0e496480f35dea60f0134d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58c9297082224bc09b280cb23e0ebcfa",
            "max": 167502836,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_424334f17f594a44a9c76dc69818c92d",
            "value": 167502836
          }
        },
        "7087bb1e3cec40a3a98d52ad17234194": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da4254594e5543728e4116dffd56112c",
            "placeholder": "​",
            "style": "IPY_MODEL_37189771590c4287a13bf80249e43fe6",
            "value": " 160M/160M [00:02&lt;00:00, 70.5MB/s]"
          }
        },
        "b345a3d40e334fcd95029e638165926f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be7e2ad889814fa1a471c674f354ce9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a22ab99c0d3b4ea98e76cd7b412ec14f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "58c9297082224bc09b280cb23e0ebcfa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "424334f17f594a44a9c76dc69818c92d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "da4254594e5543728e4116dffd56112c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37189771590c4287a13bf80249e43fe6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}